---
title: "Yvonne model training"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo = T, message = FALSE, results='hide', warning=FALSE}
library(caret)
library(tidymodels)
library(splines)
library(mgcv)
library(pdp)
library(earth)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(discrim) 
library(MASS)
library(mlbench)
library(pROC)
library(klaR)
library(plotmo)
```

# Dataset split
Randomly split dataset into traning (80%) and testing data (20%).
```{r}
# load dataset
load("~/Desktop/8106 Data Science II/data science II midterm project/recovery.RData")

# remove id
data <- subset(dat, select = -id)

# set study A/B to 0/1
data$study <- ifelse(data$study == "A", 0, 1)


set.seed(666)
data_split <- initial_split(data, prop = 0.8)

# training data
training_data <- training(data_split)

# test data
testing_data <- testing(data_split)

# matrix of predictors
x <- model.matrix(recovery_time ~ ., training_data) [ ,-1]
y <- training_data$recovery_time
```

# Nonlinear Methods
## Multivariate Adaptive Regression Splines (MARS)
Train a multivariate adaptive regression spline (MARS) model.
```{r}
mars_grid <- expand.grid(degree = 1:4, 
                         nprune = 2:38)

ctrl1 <- trainControl(method = "repeatedcv", number = 10)

set.seed(666)
mars.fit <- train(x, y,
                  method = "earth",
                  tuneGrid = mars_grid,
                  trControl = ctrl1)

ggplot(mars.fit)

mars.fit$bestTune

coef(mars.fit$finalModel) 

```
The regression function is f(x) = -3.1983530 + 6.3999877 * h(31-bmi) + 25.6820131 * h(bmi-31) * study + 7.9260754 * h(bmi-25.2) - 0.6277843 * h(weight-86.4) * h(bmi-31)

The test error is 279.0367.
```{r}
# prediction
mars.pred <- predict(mars.fit, newdata = testing_data)
#test error
mean((mars.pred - testing_data[, "recovery_time"])^2)
```

## Generalized additive model (GAM)
```{r}
set.seed(666)
gam.fit <- train(x, y,
                 method = "gam",
                 tuneGrid = data.frame(method = "GCV.Cp", select = c(TRUE,FALSE)),
                 trControl = ctrl1)

gam.fit$bestTune

gam.fit$finalModel
```
The GAM model include all the predictors.

```{r}
plot(gam.fit$finalModel)
```
According to plots, variables `age`, `SBP`, `LDL`, and `weight` are more likely to have linear relationship while variables `bmi` and `height` are more likely to have nonlinear relationship.

The test error is 274.7048.
```{r}
# Create dummy variables for race and smoke
testing_dummy <- testing_data %>%
  mutate(race2 = ifelse(race == 2, 1, 0),
         race3 = ifelse(race == 3, 1, 0),
         race4 = ifelse(race == 4, 1, 0),
         smoking1 = ifelse(race == 1, 1, 0),
         smoking2 = ifelse(race == 2, 1, 0))

# prediction
gam.pred <- predict(gam.fit, newdata = testing_dummy)
#test error
mean((gam.pred - testing_dummy[, "recovery_time"])^2)
```

# Linear Methods
## lasso model
```{r}
set.seed(666)
lasso.fit <- train(recovery_time ~ .,
                   data = training_data,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1,
                                          lambda = exp(seq(5, -25, length = 100))),
                   trControl = ctrl1)

plot(lasso.fit, xTrans = log)

lasso.fit$bestTune

# coefficients in the final model w/ min CV MSE
coef_min <- coef(lasso.fit$finalModel, s = lasso.fit$bestTune$lambda)

sum(coef_min[-1, ] != 0)
```
There are 17 variables in the final lasso model. The selected tunning parameter applying minimum CV MSE: lambda = 0.00912288.

The test error is 298.3016
``` {r}
# prediction
lasso.pred <- predict(lasso.fit, newdata = testing_data)

# test error
mean((lasso.pred - testing_data[, "recovery_time"])^2)
```

## elastic net model
```{r}
set.seed(666)
# min CV MSE
enet.fit <- train(recovery_time ~ .,
                data = training_data,
                method = "glmnet",
                tuneGrid = expand.grid(alpha = seq(0, 1, length = 21),
                                       lambda = exp(seq(-25, 5, length = 100))),
                trControl = ctrl1)
enet.fit$bestTune
coef(enet.fit$finalModel, enet.fit$bestTune$lambda)
enet.pred <- predict(enet.fit, newdata = testing_data)

# plot
myCol <- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol))

plot(enet.fit, par.settings = myPar)

# test error
mean((enet.pred - testing_data[, "recovery_time"])^2)
```
alpha = 0.15, lambda = 0.00367552, test error = 297.1642

## partial least squares (PLS)
```{r}
set.seed(666)
pls.fit <- train(x, y,
                 method = "pls",
                 tuneGrid = data.frame(ncomp = 1:15),
                 trControl = ctrl1,
                 scale = TRUE)
x2 <- model.matrix(recovery_time ~ ., testing_data)[, -1]
predy.pls <- predict(pls.fit, newdata = x2)

# test error
mean((testing_data$recovery_time - predy.pls)^2)

ggplot(pls.fit,highlight = TRUE)

# ncomp
pls.fit$finalModel$ncomp


```
The test error is 301.0147
There are 11 components included in the model.

# Classification
According to CDC, most patients appear to recover from acute COVID-19 illness within 4 weeks, which is 28 days. Classify outcome `recovery_time` into < 28 days: normal, >= 28 days: long.
```{r}
training_class <- training_data %>%
  mutate(recovery_time = as.factor(ifelse(recovery_time < 28, "normal", "long")))

training_class <- training_data %>%
  mutate(recovery_time = case_when(
    recovery_time < 28 ~ "normal",
    recovery_time >= 28 & recovery_time <= 84 ~ "long",
    recovery_time > 84 ~ "verylong"
  )) %>%
  mutate(recovery_time = factor(recovery_time, levels = c("normal", "long", "verylong")))

rm(training_class)
```
## LDA
```{r}
# Exploratory analysis: LDA based on every combination of two variables
partimat(recovery_class ~ age + height + weight + bmi + SBP + LDL, 
         data = training_class, method = "lda")
```

```{r}
set.seed(666)
ctrl2 <- trainControl(method = "repeatedcv", repeats = 10,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)

# Convert non-numeric columns to numeric
training_class[, 1:14] <- lapply(training_class[, 1:14], as.numeric)

model.lda <- train(x = training_class[, 1:14],
                   y = training_class$recovery_time,
                   method = "lda",
                   metric = "ROC",
                   trControl = ctrl2)

testing_data[, 1:14] <- lapply(testing_data[, 1:14], as.numeric)

lda.pred <- predict(model.lda, newdata = testing_data, type = "prob")
head(lda.pred)
```

# QDA
```{r}
model.qda <- train(x = training_class[, c("age", "height", "weight", "bmi", "SBP", "LDL")],
                   y = training_class$recovery_time,
                   method = "lda",
                   metric = "ROC",
                   trControl = ctrl2)

qda.pred <- predict(model.qda, newdata = testing_data, type = "prob")
head(lda.pred)
```

# Model Comparison
```{r}
resamp <- resamples(list(mars = mars.fit,
                      gam = gam.fit,
                      enet = enet.fit,
                      lasso = lasso.fit,
                      pls = pls.fit,
                      LDA = model.lda,
                      QDA = model.qda))

resamp <- resamples(list(mars = mars.fit,
                      
                      LDA = model.lda,
                      QDA = model.qda))

summary(resamp)

bwplot(resamp, metric = "RMSE")
```

