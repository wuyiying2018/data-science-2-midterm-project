---
title: "Midterm Result" 
author: "Yiying Wu (yw3996)"
output:
  pdf_document:
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \pagestyle{fancy}
- \fancyhead[L]{Data Science 2 Midterm}
- \fancyhead[R]{Yiying Wu (yw3996)}
- \fancypagestyle{plain}{\pagestyle{fancy}}
--- 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```


```{r,echo = FALSE}
## R packages

library(tidyverse)
library(corrplot)
library(caret)
library(tidymodels)
library(gtsummary)
library(pls)
```


```{r,echo = FALSE}
## Input dataset
load("./recovery.RData")
```


```{r,echo = FALSE}
## recode categorical data

dat <-dat %>% 
  mutate(
    ## gender
    male=gender,
    ## race
    race=relevel(as.factor(race), ref ="1"),
    ## smoking
    smoking=relevel(as.factor(smoking), ref ="0"),
    ## study
    study=relevel(as.factor(study), ref ="A")
  )
```


## Exploratory analysis and data visualization

In this section, use appropriate visualization techniques to explore the dataset and identify any patterns or relationships in the data.

**Summary statistics**
```{r,echo = FALSE}
summ_dat<-dat%>%
  select(-id,-male)%>%
  mutate(
    gender = factor(case_when(
      gender == "1" ~ "female",  
      gender == "0" ~ "male"),
      levels = c("male", "female")
    ),
    race= factor(case_when(
      race == "1" ~ "White",  
      race == "2" ~ "Asian",
      race == "3" ~ "Black",
      race == "4" ~ "Hispanic"),
      levels = c("White", "Asian","Black","Hispanic")
    ),
    smoking=factor(case_when(
      smoking == "0" ~ "Never smoked",  
      smoking == "1" ~ "Former smoker",
      smoking == "2" ~ "Current smoker"),
      levels = c("Never smoked", "Former smoker","Current smoker")
    ),
    hypertension=factor(case_when(
      hypertension == "0" ~ "No",  
      hypertension == "1" ~ "Yes"),
      levels = c("No", "Yes")
    ),
    diabetes=factor(case_when(
      diabetes == "0" ~ "No",  
      diabetes == "1" ~ "Yes"),
      levels = c("No", "Yes")
    ),
    vaccine=factor(case_when(
      vaccine == "0" ~ "Not vaccinated",  
      vaccine == "1" ~ "Vaccinated"),
      levels = c("Not vaccinated", "Vaccinated")
    ),
    severity=factor(case_when(
      severity == "0" ~ "Not severe",  
      severity == "1" ~ "Severe"),
      levels = c("Not severe", "Severe")
    )
    )

summ_dat %>% 
  tbl_summary() %>% 
  bold_labels()%>%
  as_gt(include = everything()) %>%
  gt::tab_header("Table 1: Summary of Dataset")
```

**Visualizations for the numerical variables**

```{r,echo = FALSE}
ggplot(summ_dat, aes_string(x = "age")) + geom_histogram(bins = 30) + ggtitle("Histogram of age")

ggplot(summ_dat, aes_string(x = "height")) + geom_histogram(bins = 30) + ggtitle("Histogram of height")

ggplot(summ_dat, aes_string(x = "weight")) + geom_histogram(bins = 30) + ggtitle("Histogram of weight")

ggplot(summ_dat, aes_string(x = "bmi")) + geom_histogram(bins = 30) + ggtitle("Histogram of bmi")

ggplot(summ_dat, aes_string(x = "SBP")) + geom_histogram(bins = 30) + ggtitle("Histogram of SBP")

ggplot(summ_dat, aes_string(x = "LDL")) + geom_histogram(bins = 30) + ggtitle("Histogram of LDL")

ggplot(summ_dat, aes_string(x = "recovery_time")) + geom_histogram(bins = 30) + ggtitle("Histogram of recovery_time")
```

**correlation plot**

```{r,echo = FALSE}
numerical_data <- summ_dat[, sapply(summ_dat, is.numeric)]
correlation_matrix <- cor(numerical_data)
corrplot::corrplot(correlation_matrix, method = "color", addCoef.col = "black", tl.col = "black", tl.srt = 45, insig = "blank" , number.cex = 0.7, diag = FALSE)
```


## Model training

In this section, describe the models you used to predict the time to recovery from COVID-19. Briefly state the assumptions made by using the models. Provide a detailed description of the model training procedure and how you obtained the final model.

**Outcome: recovery_time**

### Partition the dataset into two parts: training data (80%) and test data (20%).
```{r}
set.seed(666)
data_split <- initial_split(dat, prop = 0.8)

# Extract the training and test data
training_data <- training(data_split)%>% select(-id,-gender)
x_train <- training_data %>% select(-recovery_time)
y_train <- training_data$recovery_time

testing_data <- testing(data_split)%>% select(-id,-gender)
x_test <- testing_data %>% select(-recovery_time)
y_test <- testing_data$recovery_time

# ctrl
ctrl <- trainControl(method = "cv", number = 10)
```

### Multivariate Adaptive Regression Spline (MARS) Model
```{r}
set.seed(666)
model.mars <- train(x = x_train,
                    y = y_train,
                    method = "earth", # earth is for mars
                    tuneGrid = expand.grid(degree = 1:4, 
                                           nprune = 2:20),
                    trControl = ctrl)

# degree from 1~4 is sufficient
# nprune can be larger than the number of predictors, make it as large as possible

plot(model.mars)
# both number of terms and product degree are upper bounds

# best tune
model.mars$bestTune

coef(model.mars$finalModel)
```

**test error**

```{r}
mars.pred <- predict(model.mars, newdata = x_test)
test_error_mars <- mean((mars.pred - y_test)^2)
test_error_mars
RMSE_mars <- sqrt(test_error_mars)
RMSE_mars
```

The MSE of MARS model is `r round(test_error_mars,3)`.

### Generalized Additive Model (GAM)
```{r,warning=FALSE}
set.seed(666)
model.gam <- train(x = x_train,
                   y = y_train,
                   method = "gam",
                   trControl = ctrl)

model.gam$bestTune

model.gam$finalModel
# degree of freedom=1 means linear

# Plotting
plot(model.gam$finalModel)

```

```{r}
# compute and report the test error
predictions <- predict(model.gam, x_test)
test_error <- mean((predictions - y_test)^2) # Mean Squared Error (MSE)
test_error # Reporting the test error
```

The MSE of GAM model is `r round(test_error,3)`

### lasso model
```{r}
set.seed(666)
lasso.fit <- train(recovery_time ~ .,
                   data = training_data,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(-25, 5, length = 100))),
                   trControl = ctrl)

```


Here's the selected tuning parameter when the minimal MSE rule is applied
```{r}
lasso.fit$bestTune
```

The best tuning parameter is `r round(lasso.fit$bestTune$lambda,3)`

And the test error is
```{r}
lasso.pred <- predict(lasso.fit, newdata = testing_data)
# test error
mean((lasso.pred - testing_data$recovery_time)^2)
```

The MSE of lasso model is `r round(mean((lasso.pred - testing_data$recovery_time)^2),3)`

### Elastic net model

```{r}
set.seed(666)
enet.fit <- train(recovery_time ~ .,
                  data = training_data,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                         lambda = exp(seq(-25, 5, length = 100))),
                  trControl = ctrl)
```

Here's the selected tuning parameter

```{r}
enet.fit$bestTune
```

The best tuning parameter is `r round(enet.fit$bestTune$lambda,3)`

And the test error is

```{r}
enet.pred <- predict(enet.fit, newdata = testing_data)
# test error
mean((enet.pred - testing_data$recovery_time)^2)
```

The MSE of elastic net model is `r round(mean((enet.pred - testing_data$recovery_time)^2),3)`

### Principal components regression (PCR)

```{r}
set.seed(666) 

pcr.fit <- train(x = x_train, 
                 y = y_train,
                 method = "pcr",
                 tuneGrid = data.frame(ncomp = 1:15),
                 trControl = ctrl,
                 preProcess = c("center", "scale"))
predy.pcr <- predict(pcr.fit, newdata = x_test)


ggplot(pcr.fit, highlight = TRUE) + theme_bw()

# test MSE
mean((y_test - predy.pcr)^2)
```
The MSE of pcr model is `r round(mean((y_test - predy.pcr)^2),3)`

## Model Comparison

compare the RMSE

```{r}
resamp <- resamples(list(mars=model.mars,
                         gam=model.gam,
                         lasso=lasso.fit,
                         enet=enet.fit,
                         pcr=pcr.fit))
summary(resamp)

bwplot(resamp, metric = "RMSE")
```

The MARS model is preferred since it has a lower mean value of RMSE compared to other models. 